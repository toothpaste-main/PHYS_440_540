{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "## Due Wednesday 30 October, before class\n",
    "\n",
    "### PHYS 440/540, Fall 2024\n",
    "https://github.com/gtrichards/PHYS_440_540/\n",
    "\n",
    "\n",
    "## Problems 1&2\n",
    "\n",
    "Complete Chapters 1 and 2 in the *unsupervised learning* course in Data Camp.  The last video (and the two following code examples) in Chapter 2 are off topic, but we'll discuss those next week, so this will be a good intro.  The rest is highly relevant to this week's material.  These are worth 1000 and 900 points, respectively.  I'll be grading on the number of points earned instead of completion (as I have been), so try to avoid using the hints unless you really need them.\n",
    "\n",
    "## Problem 3\n",
    "\n",
    "This exercise will take you though an example of everything that we did this week.  Please copy the relevant import statements (below) to the cells where they are used (so that they can be run out of order).  \n",
    "\n",
    "If a question is calling for a word-based answer, I'm not looking for more than ~1 sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from astroML.density_estimation import KNeighborsDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup up the data set.  We will do both density estimation and clustering on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "#Make two blobs with 2 features and 1000 samples\n",
    "N=1000\n",
    "X,y = make_blobs(n_samples=N, centers=5, n_features=2, random_state=99)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=100, c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a gaussian kernel density estimation, including a grid search to find the best bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwrange = ____.____(____,____,____) # Test 60 bandwidths from 0.1 to 1.0 ####\n",
    "K = ____ # 5-fold cross validation ####\n",
    "grid = ____(KernelDensity(), {'bandwidth': ____}, cv=____) ####\n",
    "grid.fit(____) #Fit the histogram data that we started the lecture with.\n",
    "h_opt = ____.best_params_['bandwidth'] ####\n",
    "print(h_opt)\n",
    "\n",
    "kde = KernelDensity(kernel=____, bandwidth=____)\n",
    "kde.fit(X) #fit the model to the data\n",
    "\n",
    "u = v = np.linspace(-15,15,100)\n",
    "Xgrid = np.vstack([np.ravel(arr) for arr in np.meshgrid(u, v)]).T\n",
    "dens = np.exp(kde.score_samples(Xgrid)) #evaluate the model on the grid\n",
    "\n",
    "plt.scatter(____[:,0],____[:,1], c=dens, cmap=\"Purples\", edgecolor=\"None\") ####\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try a nearest neighbors approach to estimating the density. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What value of $k$ do you need to make the plot look similar to the one above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute density with Bayesian nearest neighbors\n",
    "k=____ ####\n",
    "nbrs = KNeighborsDensity('bayesian',____=____) ####\n",
    "nbrs.____(X) ####\n",
    "dens_nbrs = nbrs.eval(Xgrid) / N\n",
    "\n",
    "plt.scatter(Xgrid[:,0],Xgrid[:,1], c=dens_nbrs, cmap=\"Purples\", edgecolor=\"None\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do a Gaussian mixture model.  Do a grid search for between 1 and 10 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kludge to fix the bug with draw_ellipse in astroML v1.0\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def draw_ellipse(mu, C, scales=[1, 2, 3], ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # find principal components and rotation angle of ellipse\n",
    "    sigma_x2 = C[0, 0]\n",
    "    sigma_y2 = C[1, 1]\n",
    "    sigma_xy = C[0, 1]\n",
    "\n",
    "    alpha = 0.5 * np.arctan2(2 * sigma_xy,\n",
    "                             (sigma_x2 - sigma_y2))\n",
    "    tmp1 = 0.5 * (sigma_x2 + sigma_y2)\n",
    "    tmp2 = np.sqrt(0.25 * (sigma_x2 - sigma_y2) ** 2 + sigma_xy ** 2)\n",
    "\n",
    "    sigma1 = np.sqrt(tmp1 + tmp2)\n",
    "    sigma2 = np.sqrt(tmp1 - tmp2)\n",
    "\n",
    "    for scale in scales:\n",
    "        ax.add_patch(Ellipse((mu[0], mu[1]),\n",
    "                             width=2 * scale * sigma1, height=2 * scale * sigma2,\n",
    "                             angle=alpha * 180. / np.pi,\n",
    "                             **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomps = np.arange(____,____,____) # Test 10 bandwidths from 1 to 10 ####\n",
    "K = 5 # 5-fold cross validation\n",
    "grid = ____(GaussianMixture(), {'n_components': ____}, cv=____)  ####\n",
    "grid.fit(____) #Fit the histogram data that we started the lecture with.\n",
    "ncomp_opt = grid.____['n_components'] ####\n",
    "print(ncomp_opt)\n",
    "\n",
    "gmm = ____(n_components=____) ####\n",
    "gmm.fit(____)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X[:,0],X[:,1])\n",
    "\n",
    "ax.scatter(gmm.means_[:,0], gmm.means_[:,1], marker='s', c='red', s=80)\n",
    "for mu, C, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "    draw_ellipse(mu, 1*C, scales=[2], ax=ax, fc='none', ec='k') #2 sigma ellipses for each component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do you get the same answer (the same number of components) each time you run it?  (Rerun it at least 10 times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try Kmeans.  Here we will scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=____)\n",
    "scaler = ____()\n",
    "X_scaled = ____.____(X) ####\n",
    "kmeans.fit(____)\n",
    "centers=kmeans.____ #location of the clusters ####\n",
    "labels=kmeans.predict(____) #labels for each of the points ####\n",
    "centers_unscaled = scaler.____(centers) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(16, 8))\n",
    "ax[0].scatter(X[:,0],X[:,1],c=labels)\n",
    "ax[0].scatter(centers_unscaled[:,0], centers_unscaled[:,1], marker='s', c='red', s=80)\n",
    "ax[0].set_title(\"Predictions\")\n",
    "\n",
    "ax[1].scatter(X[:, 0], X[:, 1], c=y)\n",
    "ax[1].set_title(\"Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate how well we did in two other ways: a matrix and a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'predictions': labels, 'truth': y})\n",
    "ct = pd.crosstab(df[____], df[____])\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "score = ____(df[____], df[____])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the score for 3 clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use DBSCAN.  Note that outliers are flagged as `labels_=-1`, so there is one more class that you might think.\n",
    "\n",
    "Full credit if you can get a score of 0.6 or above.  Extra credit (0.1 of 5 points) for a score of 0.85 or above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n",
    "    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
    "    core_mask[dbscan.core_sample_indices_] = True\n",
    "    anomalies_mask = dbscan.labels_ == -1\n",
    "    non_core_mask = ~(core_mask | anomalies_mask)\n",
    "\n",
    "    cores = dbscan.components_\n",
    "    anomalies = X[anomalies_mask]\n",
    "    non_cores = X[non_core_mask]\n",
    "    \n",
    "    plt.scatter(cores[:, 0], cores[:, 1],\n",
    "                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n",
    "    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20, c=dbscan.labels_[core_mask])\n",
    "    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n",
    "                c=\"r\", marker=\"x\", s=100)\n",
    "    plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker=\".\")\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)\n",
    "    plt.title(\"eps={:.2f}, min_samples={}\".format(dbscan.eps, dbscan.min_samples), fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=____, min_samples=____,metric=___)\n",
    "dbscan.fit(____)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_dbscan(dbscan, X_scaled, size=100)\n",
    "n_clusters=np.unique(dbscan.labels_)\n",
    "print(len(n_clusters)) #Number of clusters found (+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({____: dbscan.labels_, ____: y})\n",
    "ct2 = pd.crosstab(df2['predictions'], df2['truth'])\n",
    "print(ct2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "score2 = homogeneity_score(df2____,df2____)\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do you think DBSCAN is having a hard time?  Think about what the Gaussian Mixture Model result showed."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
