{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks I\n",
    "G. Richards\n",
    "(2016, 2018, 2020, 2022, 2024)   \n",
    "With updates to my own class from [Stephen Taylor's class at Vanderbilt](https://github.com/VanderbiltAstronomy/astr_8070_s4\n",
    "2).\n",
    "\n",
    "I found this video series particularly helpful in trying to simplify the explanation https://www.youtube.com/watch?v=bxe2T-V8XRs. \n",
    "\n",
    "#### Reading:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 9.\n",
    "- [Geron](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_5?dchild=1&keywords=machine+learning&qid=1596499152&sr=8-5)\n",
    "- Many blogs and videos.\n",
    "- Free online book! http://neuralnetworksanddeeplearning.com/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents\n",
    "\n",
    "* [Preliminaries](#one)\n",
    "* [Neural networks](#two)\n",
    "* [Activation Functions](#three)\n",
    "* [Keras](#four)\n",
    "* [Regularization](#five)\n",
    "* [Batch Normalization](#six)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminaries <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "Before starting today, let's install some software, including the two big Deep Learning packages out there. [Tensorflow](https://www.tensorflow.org/) and [PyTorch](https://pytorch.org/)-- the rivalry essentially boils down to Google versus Facebook.  See the cells below.  Go ahead and do this while you are waiting for lecture to start.  \n",
    "\n",
    "We won't make much use of these today.  Mostly we need them for the next lecture. So, you'll have the weekend to debug if you have problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On the command line:\n",
    "    \n",
    "% pip3 install --upgrade pip\n",
    "    \n",
    "% pip3 install tensorflow --upgrade\n",
    "\n",
    "% pip3 install torch\n",
    "\n",
    "% pip3 install pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "If you have a Mac M1 (or newer) computer, have a look at\n",
    "\n",
    "[https://developer.apple.com/metal/tensorflow-plugin/](https://developer.apple.com/metal/tensorflow-plugin/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Neural Networks  <a class=\"anchor\" id=\"two\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Quick review from last time....\n",
    "\n",
    "[Artificial Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network) are a simplified computation architecture based loosely on the real neural networks found in brains. \n",
    "\n",
    "![Neuron example](https://4.bp.blogspot.com/-Z5LfY6yoIcE/U-OFKWHoAbI/AAAAAAAAAKo/ytH6BzDLeo4/s1600/Picture-533.png)\n",
    "\n",
    "In reality, what we are going to explore is a **[multi-layer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In the image below, \n",
    "- the circles on the ***left*** represent the **features/attributes** of our input data, $X$, which here is 3 dimensional.  \n",
    "- the circles in the ***middle*** represent the **neurons**. They take in the information from the input and, based on some criterion decide whether or not to \"fire\". These middle layers are called \"**hidden layers**\".\n",
    "- the collective results of the neurons in the hidden layer produce the **output**, $y$, which is represented by the circles on the ***right***, which here is 2 dimensional result.  \n",
    "- the lines connecting the circles represent the synapses.  \n",
    "\n",
    "This is a simple example with just one layer of neurons; however, there can be many layers of neurons.\n",
    "\n",
    "![Cartoon of Neural Network](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/500px-Artificial_neural_network.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are two graphics from an [article](https://towardsdatascience.com/data-science-vs-artificial-intelligence-vs-machine-learning-vs-deep-learning-9fadd8bda583) describing the relationship between artificial inteligence, machine learning and deep learning.\n",
    "\n",
    "![MLvsDL1](https://miro.medium.com/max/1060/0*R53mzDRJXZ8l6idL)\n",
    "\n",
    "![MLvsDL2](https://miro.medium.com/max/1400/0*2V2i5DbamWhswRV6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Back to understanding this architecture.\n",
    "\n",
    "![Cartoon of Neural Network](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/500px-Artificial_neural_network.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "What the neural network does is to learn the weights of the synapses that are needed to produce an accurate model of $y_{\\rm train}$.\n",
    "\n",
    "![Ivezic Figure 9.17](https://www.astroml.org/_images/fig_neural_network_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "If you haven't already done so, now is a good time to watch the first 3 videos in this series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('https://www.youtube.com/watch?v=GlcnxUlrtek&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU&index=4&ab_channel=WelchLabs', width=1000, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Activation Functions  <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "The **[Activation function](https://en.wikipedia.org/wiki/Activation_function)** controls how much \"signal\" it takes for a neuron to \"fire\".  The more signal, the more likely the neuron will fire. See https://mlfromscratch.com/activation-functions-explained/#/\n",
    "\n",
    "The cells below show different activation functions, using the same visualization as we used for loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mathematical formulas for activation functions\n",
    "  \n",
    "def binary(raw_model_output):\n",
    "    return np.where(raw_model_output < 0, \n",
    "                    0, \n",
    "                    1)  \n",
    "\n",
    "def sigmoid(raw_model_output):\n",
    "    return 1.0 / (1 + np.exp(-raw_model_output))\n",
    "\n",
    "def ReLU(raw_model_output):\n",
    "    return np.where(raw_model_output < 0, \n",
    "                    0, \n",
    "                    raw_model_output)\n",
    "\n",
    "def LReLU(raw_model_output):\n",
    "    alpha=0.1\n",
    "    return np.where(raw_model_output < 0, \n",
    "                    alpha*raw_model_output, \n",
    "                    raw_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a grid of values and plot\n",
    "grid = np.linspace(-3,3,1000)\n",
    "#plt.plot(grid, log_loss(grid), label='logistic')\n",
    "plt.plot(grid, binary(grid), \"k\", label='binary')\n",
    "plt.plot(grid, sigmoid(grid), \"b\", label='sigmoid')\n",
    "#plt.plot(grid, ReLU(grid), label='ReLU')\n",
    "#plt.plot(grid, LReLU(grid), label='LReLU')\n",
    "#plt.plot(grid, l2(grid), label='L2')\n",
    "#plt.plot(grid, l1(grid), label='L1')\n",
    "\n",
    "plt.fill_between([0,3], y1=-1, y2=3, \n",
    "                 color=\"b\", alpha=0.2)\n",
    "plt.fill_between([-3,0], y1=-1, y2=3, \n",
    "                 color=\"r\", alpha=0.2)\n",
    "plt.xlim([-3,3])\n",
    "plt.ylim([-1,3])\n",
    "plt.xlabel(\"z\",fontsize=12)\n",
    "plt.title(\"don't fire                 fire\",fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\"*On vs. Off*\" activation actually isn't quite true or what we want. The way your eyes work is that you need 1-10 photons to trigger a *rod*, but several rods must be triggered to send a signal to the brain. The sigmoid activation function captures the probabilistic nature of neuron firing. More importantly it is differentiable, so can be used for backpropagation.\n",
    "\n",
    "Another important aspect of non-linear activation functions is that they are what allow neural networks to solve non-linear problems.  If we used a strictly linear activation function, then we could only solve linear problems.  That is, you could fit a straight line, but not an exponential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "If you wanted to make the activation function a parameter, here's how to find out what your options are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Different activation functions that are available\n",
    "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Vanishing and Exploding Gradients\n",
    "\n",
    "Neural network research suffered significant limitations and problems at the hands of the [vanishing and exploding gradients](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) problem.  We won't go into detail there except to say that ***around 2010 there were suggestions for different activation functions that were published.***  \n",
    "\n",
    "- For example, the **[Rectified Linear Unit (ReLU) activation function](https://www.wikiwand.com/en/Rectifier_(neural_networks))** is another commonly used activation function as it solves the vanishing gradient problem (since the gradient is only 0 or 1).\n",
    "$${\\rm ReLU}(z) = max(0,z)$$\n",
    "\n",
    "\n",
    "- It isn't ideal both because the derivative is 0 for $z<0$ and it can end up producing dead nodes. However, it is fast and the resulting sparsity can be good (sort of like regularization).\n",
    "\n",
    "\n",
    "- A number of papers since 2015 describe various improvements, including the **Leaky ReLU**, the **exponential linear unit (ELU)**, **scaled exponential linear unit (SELU)**, and **Swish**.\n",
    "\n",
    "\n",
    "Note that the activation functions can be different in different layers.  For example, for regression, one typically doesn't use any activation function in the output layer as including one would restrict the range of possible outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a grid of values and plot\n",
    "grid = np.linspace(-3,3,1000)\n",
    "#plt.plot(grid, log_loss(grid), label='logistic')\n",
    "#plt.plot(grid, binary(grid), \"k\", label='binary')\n",
    "#plt.plot(grid, sigmoid(grid), \"b\", label='sigmoid')\n",
    "plt.plot(grid, ReLU(grid), \"b--\", label='ReLU')\n",
    "plt.plot(grid, LReLU(grid), \"orange\", label='LReLU')\n",
    "#plt.plot(grid, l2(grid), label='L2')\n",
    "#plt.plot(grid, l1(grid), label='L1')\n",
    "\n",
    "plt.fill_between([0,3], y1=-1, y2=3, \n",
    "                 color=\"b\", alpha=0.2)\n",
    "plt.fill_between([-3,0], y1=-1, y2=3, \n",
    "                 color=\"r\", alpha=0.2)\n",
    "plt.xlim([-3,3])\n",
    "plt.ylim([-1,3])\n",
    "plt.xlabel(\"z\",fontsize=12)\n",
    "plt.title(\"don't fire                       fire\",fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Some general guidance on activation functions:\n",
    "    \n",
    "* **Use sigmoid for output of binary classification (with binary cross entropy loss)**\n",
    "\n",
    "\n",
    "* **Use ReLU for layers other than output (at least to start with because it is faster)**\n",
    "\n",
    "\n",
    "* **Use softmax for output with more than 2 classes (with categorical cross entropy loss)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Faster Optimizers\n",
    "\n",
    "We aren't going to talk about optimizers, but it might be useful for you to have some options to feed into a search for the best parameter using cross validation.  For example **`['mse', 'adam', 'sgd', 'adagrad']`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "If we want to do anything more complicated, we'll need to use something other than Scikit-Learn.  \n",
    "\n",
    "Enter Keras and TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Keras  <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "**[Keras](https://keras.io/)** is a **deep learning** API. Essentially it is Scikit-Learn for deep neural networks.\n",
    "\n",
    "Keras needs a computational backend to handle the heavy computation.  Three popular (open sources) deep learning libraries are [TensorFlow](https://www.tensorflow.org/), Microsoft Cognitive Toolkit, and [Theano](http://www.deeplearning.net/software/theano/). TensorFlow now comes bundled with a version of Keras and that's what we'll use here (actually TensorFlow 2). PyTorch is another option.  Section 9.8 of Ivezic includes examples using both `keras` and `torch`.\n",
    "\n",
    "If you apply for a data-science job in industry, knowing one of these tools might be the most useful thing for you to have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In short, for neural networks:\n",
    "\n",
    "> numpy -> tensorflow\n",
    "\n",
    "> sklearn -> keras\n",
    "\n",
    "In the same way that you can build a linear regression algorithm in numpy without using sklearn, you can build a neural network algorithm (not to mention linear regression) in tensorflow without using keras.  But just as sklearn makes our life easier, so too does keras.  \n",
    "\n",
    "Keras has Sequential and Functional APIs.  We will just use Sequential in our examples.\n",
    "\n",
    "Just as I'm teaching you about sklearn and not numpy, I have no intention of teaching you tensorflow.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The following cells are from Geron, Chapter 10, see \n",
    "https://github.com/ageron/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb\n",
    "\n",
    "I'll indicate below when I switch chapters.\n",
    "\n",
    "We'll start by introducing the Fashion MNIST data set, which is as common for learning about neural networks as the MNIST are for machine learning in general.  This data set is a huge collection of images of items of clothing.  We can also use the MNIST data too.  \n",
    "\n",
    "The next cells load the data, define test, trainging, and validation sets; normalize the data; display an example image; list the possible target values ($y$), and show a 4x10 grid of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "print(X_train_full.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0], cmap=\"binary\", origin='upper') #Origin controls right-side up\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Take a look at some of the other entries in the training data.  Also see what happens when you remove the `origin='upper'` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Let's make a list of class names that we can refer to.\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot 4x10 array of images from the Fashion MNIST database\n",
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\", origin='upper')\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now we'll build a \"simple\" neural network that classifies an unknown image (preprocessed to have the same image and color scale) into one of these 10 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session() #Make sure that we are starting a new model and not adding to an earlier one\n",
    "np.random.seed(42) #Set the numpy and tensorflow random seeds so that we all get the same answer\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=[28, 28]),  # Define the input layer explicitly\n",
    "    keras.layers.Flatten(),              # Flatten the input\n",
    "    keras.layers.Dense(300, activation=\"relu\"),  # First hidden layer\n",
    "    keras.layers.Dense(100, activation=\"relu\"),  # Second hidden layer\n",
    "    keras.layers.Dense(10, activation=\"softmax\") # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "model = keras.models.Sequential() #Instantiate a sequential model\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28])) #Define the input layer\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\")) #First hidden layer\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\")) #Second hidden layer\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\")) #Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "Need to define the input size/shape for the first layer.  The others know how to talk to each other.  The last layer sets the number of outputs.  Need softmax here because there are 10 categories.  If just one output or two, then a different activation function would be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Let's see what we just built\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The number of parameters in each layer is one for each weight that connects each node of the layer to the previous layer, plus the number of neurons in the layer to account for the constant \"bias\" parameter (e.g., 784x300+300 = 235500).\n",
    "\n",
    "Note that \"Dense\" here means \"fully connected\".  That is, each node in one layer is connected to each node in the next layer.  This doesn't always have to be the case.  The layers can instead be \"Sparse\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We can access (or set) the weights with `get_weights()` and `set_weights()` as follows.  Note that the initial weights are set randomly and the bias values are initially zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]\n",
    "weights, biases = hidden1.get_weights()\n",
    "#Weights set to random numbers to break degeneracy\n",
    "#Biases are set to 0\n",
    "print(weights.shape,weights,biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "For neural networks we can't just fit the model after we instantiate it.  We need to compile first--specifying the loss function, the optimizer, and any desired metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#For binary classification instead do this\n",
    "#(and change the activation function of the output layer from \"softmax\" to \"sigmoid\"\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now we can go ahead and fit the model with our training data.  The fit method will output a bunch of useful diagnostics that we'll save to `history` for plotting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "Comment on what is happening with loss, val_loss, and accuracy.\n",
    "\n",
    "Validation flattens around 30 if we let it keep running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Our metrics of interest were printed at each epoch (each pass through the neural network to update the weights), but it is easier to just look at a plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(12, 8))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.gca().set_xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "As was the case for model training before, we want to make sure that we aren't overtraining, which would be indicated by the training loss diverging from the validation loss.  Note that, instead of creating a specific validation set, we can also pass `validation_split=0.1` as a parameter during the fitting step to split off 10% of the data for validation.\n",
    "\n",
    "Now we will `evaluate` the model using the test set to determine the expected level of error on unknown data.    The usual `predict` method then can be used to make predictions.  [Note: I'm not sure the reason for this structure.  In regular ML we would `predict` on the test set and the compute metrics with the output values.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Evaluate the test set\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Predict values for first 3 test objects\n",
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new).round(2)\n",
    "print(y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "So, you can see that the output is a probability that the object belongs to each class (which has to sum to 1 across all the classes).  If we just want an \"answer\", we assign it to the class with the highest probability (which we do here with a simple *argmax*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_new), axis=-1) #New way\n",
    "print(y_pred)\n",
    "print(np.array(class_names)[y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Let's plot those and see if the predictions make sense.\n",
    "plt.figure(figsize=(7.2, 2.4))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, 3, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\", origin='upper')\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_test[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "#save_fig('fashion_mnist_images_plot', tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Note that after you fit your model, you can save it and reload it at some later time (which is good because some models might take hours to train!)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#Save model\n",
    "model.save(\"my_keras_model.h5\")\n",
    "\n",
    "#Reload model\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Save model\n",
    "model.save(\"my_keras_model.keras\")\n",
    "\n",
    "#Reload model\n",
    "model = keras.models.load_model(\"my_keras_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's predict the values for 10 random objects.  Display them with their actual labels first, then predict and display the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx10 = np.random.choice(np.arange(len(y_test)), size=10, replace=False)\n",
    "X_new = X_test[idx10]\n",
    "#y_pred = model.predict_classes(X_new)\n",
    "y_pred = np.argmax(model.predict(X_new), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Correct answers (y_test)\n",
    "plt.figure(figsize=(15, 5))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, 10, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\", origin='upper')\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_test[idx10[index]]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Predicted answers (y_pred)\n",
    "plt.figure(figsize=(15, 5))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, 10, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\", origin='upper')\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_pred[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Here's a regression example instead of classification, using the California housing data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "#X_new = X_test\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test,y_pred)\n",
    "plt.xlabel(\"Actual Value [x$1000]\")\n",
    "plt.ylabel(\"Predicted Value [x$1000]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Regularization  <a class=\"anchor\" id=\"five\"></a>\n",
    "\n",
    "Just as we can use regularization for standard regression and classification tasks, so too can we with neural networks.  \n",
    "\n",
    "Not only can we apply the usual $L1$ (LASSO) or $L2$ (Ridge) regularization techniques, we can also use **dropout** which, as the name indicates, causes some neurons to be temporarily \"dropped out\" during training (usually by setting some probability for that to happen, typically 10-50%). After training, all of the neurons are used.\n",
    "\n",
    "*Geron* explains this in terms of a company needing to try to figure out how to adapt to a crucial employee being out sick for a period of time.  In the end, it can make the company stronger as more people (neurons) are able to handle certain parts of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "One form of regularization is  **early stopping**.  This will determine when there hasn't been any improvement in the validation set for `patience` epochs and stop the fitting.  It also uses the \"best\" weights and just the \"last\" weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[early_stopping_cb])\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Note that we told it that it could run for as many as 100 epochs if it wanted to, but it stopped after fewer.\n",
    "\n",
    "Now we can plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(12, 8))\n",
    "plt.grid(True)\n",
    "#plt.gca().set_ylim(0, 1)\n",
    "plt.gca().set_xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Here's an example of how to do regularization with [dropout](https://en.wikipedia.org/wiki/Dilution_(neural_networks)).\n",
    "\n",
    "![DropoutDiagram](https://miro.medium.com/max/1400/1*tvcv2PT3cBAmUtZzQTmjeQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Back to the fashion MNIST data\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dropout example\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2), #20% chance of neuron stuck in off position\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Batch normalization  <a class=\"anchor\" id=\"six\"></a>\n",
    "\n",
    "Just as it is often necessary to normalize or standardize our features, sometimes it is helpful to do the same to the output of the hidden layers.  This is called **[batch normalization](https://en.wikipedia.org/wiki/Batch_normalization)** and is done before passing the data to the activation function.  It make the process more stable and can also make it faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Here we also turn off the bias parameter because it ends up not being needed and just wastes a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now let's use cross validation to determine the best options.  Here we have so many that we'll use [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) instead of `GridSearchCV`.\n",
    "\n",
    "First we need to do some preprocessing to get sklearn and keras to talk nicely to each other.\n",
    "\n",
    "Start by building the model in a way that the hyperparameters are themselves parameters, then wrap that model in a way that we can use it within sklearn.\n",
    "\n",
    "See\n",
    "https://github.com/ageron/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=10, learning_rate=3e-3, input_shape=[28,28]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=input_shape))\n",
    "    for layer in range(n_hidden): #Because the number of hidden layers is a parameter\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\")) #Output layer\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Actually, we are going to skip the next few cells as something has changed and I don't want to break everything else!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "keras_class = keras.wrappers.scikit_learn.KerasClassifier(build_model)\n",
    "#Note that there is also a KerasRegressor for regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "`keras_class` can now be used with in sklearn in the same way as any other classifier.  For example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    #\"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_hidden\": (0, 1),\n",
    "    #\"n_neurons\": np.arange(1, 30), #This would take too long for class!\n",
    "    \"n_neurons\": (5,10),\n",
    "    #\"learning_rate\": reciprocal(3e-4, 3e-2) #Not quite sure what this does, but it makes it take forever!\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_class, param_distribs, n_iter=10, cv=3, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "But that at least gives you something to start with if you are interested in trying it for real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We could try this on the MNIST digits data too (but won't)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.\n",
    "X_new = X_test[:10]\n",
    "y_new = y_test[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "plt.imshow(X_train[0], cmap=\"binary\", origin=\"upper\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Note that we are doing 3-fold cross validation, so the validation set isn't being used for training, just for early stopping."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "####Don't run this during class!  It will take too long!####\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100, \\\n",
    "                  validation_data=(X_valid,y_valid),\\\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "print(rnd_search_cv.best_params_)\n",
    "print(rnd_search_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "[m for m in dir(keras.layers) if \"relu\" in m.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advanced Topics in Neural Networks with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Recent interest in neural networks surged in 2012 when a team using a deep [convolutional neural network (CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network) achieved record results classifying objects in the [ImageNet](http://image-net.org/) data set.\n",
    "\n",
    "The idea behind CNNs is inspired by human visual perception.  Each neuron in your visual cortex doesn't \"see\" all of what your eye can see at once and some neuron are more sensitive to one pattern over another (e.g., horizontal lines vs. vertical lines).  \n",
    "\n",
    "Moreover, the simplest deeply connected neural networks would choke on \"real\" data which has far more than 28x28 pixels and would require following tens of millions of connections.  So we use a combination of \"convolution\" and \"pooling\" to reduce the dimensionality of the data first.\n",
    "\n",
    "![Ivezic Figure 9.19](https://www.astroml.org/_images/fig_cnn_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Convolutional Layers\n",
    "\n",
    "In a convolutional layer, each neuron is not connected to each neuron in the previous layer, but only those that are within its \"field of view\" as defined by a kernel (filter).  We slide the kernel over the input layer and the value in the next layer depends only on those pixels.\n",
    "\n",
    "![Convolutional Layer](https://miro.medium.com/max/1400/1*wLlXFtWI--7knyQT2wlhMg.png)\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/4800/1*GcI7G-JLAQiEoCON7xFbhg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "See Geron, Figures 14-3, 14-4, and 14-5 for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Choosing a filter (kernel) with a certain pattern can help recognize certain types of features (like horizontal or vertical lines).\n",
    "\n",
    "![vertical filter](https://miro.medium.com/max/1338/1*7IEbBib7Yc7qST5IGQoYXA.jpeg)\n",
    "\n",
    "![horizontal filter](https://miro.medium.com/max/1238/1*PSSAaH2pZbl5bK3Ef_zk4A.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The purpose of the convolutional layers is really to capture high-level features in the images, like edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Pooling Layers\n",
    "\n",
    "It is common to follow the convolutional layer by a so-called \"pooling layer\", essentially to reduce the amount of data that needs to be processed.  The full architecture of a CNN might look something like this:\n",
    "https://www.researchgate.net/profile/Xian_Wei2/publication/331986652/figure/fig1/AS:740547106988032@1553571597647/The-classic-structure-of-CNN-It-consists-of-two-modules-Feature-extraction-module-and.ppm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Where the pooling layers are reducing the number of pixels by averaging, summing, taking the max, etc.:\n",
    "\n",
    "![Pooling example](https://miro.medium.com/max/1000/1*ydNsGDxMldAiq7b96GDQwg.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "See Geron Figures 14-8, 14-9, and 14-11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "When you are done, the output gets feed into a regular, fully connected neural network which outputs the predictions.\n",
    "\n",
    "This is clearly much more sophisticated than our basic perceptron. \"Deep\" networks consist of tens of layers with thousands of neurons. These large networks have become usable thanks to two breakthroughs: the use of sparse layers and the power of graphics processing units (GPUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The sparse layers or convolutional layers in a deep network contain a large number of hidden nodes but very few synapses. The sparseness arises from the relatively small size of a typical convolution kernel (15x15 is a large kernel), so a hidden node representing one output of the convolution is connected to only a few input nodes. Compare this the our previous perceptron, in which every hidden node was connected to every input node.\n",
    "\n",
    "Even though the total number of connections is greatly reduced in the sparse layers, the total number of nodes and connections in a modern deep network is still enormous. Luckily, training these networks turns out to be a great task for GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "For further study, there are lots of resources for CNNs online.\n",
    "    \n",
    "Now let's see a worked example of a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "Note: Output of convolution is a \"feature map\".  Number of weights needed (which are computed from backpropagation is just the kernel size times the number of layers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### CNN Example using Fashion MNIST\n",
    "\n",
    "See https://github.com/ageron/handson-ml2/blob/master/14_deep_computer_vision_with_cnns.ipynb\n",
    "\n",
    "We'll start with the Fashion MNIST data set. The next cells load the data, define test, trainging, and validation sets; normalize the data; display an example image; list the possible target values (\n",
    "), and show a 4x10 grid of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session() #Make sure that we are starting a new model and not adding to an earlier one\n",
    "np.random.seed(42) #Set the numpy and tensorflow random seeds so that we all get the same answer\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Normalize and prepare the data to pass into a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "X_mean = X_train.mean(axis=0, keepdims=True)\n",
    "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_valid = (X_valid - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Need to reshape for CNN\n",
    "X_train = X_train[:, :, :, np.newaxis]\n",
    "X_valid = X_valid[:,  :, :, np.newaxis]\n",
    "X_test = X_test[:, :, :, np.newaxis]\n",
    "\n",
    "print(len(X_train))\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The next cell looks like a lot, but really it is just a sequential declaration of different types of layer in the network. It is a combination of **2D Convolution layers** (feature finding), **Max Pooling layers** (data reduction), followed by some **Dense layers** (fully-connected layers at the end of the network for classification) and some **Dropout** incorporated to strengthen the overall network and make sure all the neurons are actually pulling their weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=[28, 28, 1]),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=7, activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),  \n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n",
    "    keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=10, activation='softmax'),\n",
    "])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "Note that \"same\" means with zero padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**That's right...your CNN has ~1.5 million parameters that you are going to try to optimize on your machine...**\n",
    "\n",
    "We'll only go through one epoch of optimization, but hopefully that will be enough to get us above $50\\%$ accuracy."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "####Do NOT run during class.  It will take too long!####\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now let's see how well this model does."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "score = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict values for first 10 test objects"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "X_new = X_test[:10] # to represent \"new\" images\n",
    "print(X_new.shape)\n",
    "\n",
    "y_proba = model.predict(X_new)\n",
    "print(y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "So, you can see that the output is a probability that the object belongs to each class (which has to sum to 1 across all the classes).  If we just want an \"answer\", we assign it to the class with the highest probability (done here with a simple *argmax*)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
    "print(y_pred)\n",
    "print(np.array(class_names)[y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use the `history` output to see the improvement in performance with each epoch."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "accuracy = history_dict['accuracy']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    "  \n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "#\n",
    "# Plot the model accuracy vs Epochs\n",
    "#\n",
    "ax[0].plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "ax[0].plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "ax[0].set_title('Training & Validation Accuracy', fontsize=16)\n",
    "ax[0].set_xlabel('Epochs', fontsize=16)\n",
    "ax[0].set_ylabel('Accuracy', fontsize=16)\n",
    "ax[0].legend()\n",
    "#\n",
    "# Plot the loss vs Epochs\n",
    "#\n",
    "ax[1].plot(epochs, loss_values, 'bo', label='Training loss') \n",
    "ax[1].plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "ax[1].set_title('Training & Validation Loss', fontsize=16)\n",
    "ax[1].set_xlabel('Epochs', fontsize=16)\n",
    "ax[1].set_ylabel('Loss', fontsize=16)\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "With image data one of the cool things that we can do is to visualize what the kernel in a given convolutional layer is actually doing.  But that's beyond the scope of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "####Need sdss_images_1000.npy for this to work####\n",
    "\n",
    "#Ivezic v2, Figure 9.20, edits by GTR\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "plt.rcParams['axes.xmargin'] = 0.05\n",
    "plt.rcParams['axes.ymargin'] = 0.05\n",
    "\n",
    "\n",
    "def read_savefile(filename):\n",
    "    '''Read npy save file containing images or labels of galaxies'''\n",
    "    return np.load(filename)\n",
    "\n",
    "\n",
    "def CNN(img_channels, img_rows, img_cols, verbose=False):\n",
    "    '''Define CNN model for Nair and Abraham data'''\n",
    "\n",
    "    # some hyperparamters you can chage\n",
    "    dropoutpar = 0.5\n",
    "    nb_dense = 64\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 6, 6, border_mode='same',\n",
    "                            input_shape=(img_rows, img_cols, img_channels)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(64, 5, 5, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Convolution2D(64, 5, 5, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Convolution2D(128, 2, 2, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nb_dense, activation='relu'))\n",
    "    model.add(Dropout(dropoutpar))\n",
    "    model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "    print(\"Compilation...\")\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    print(\"... done!\")\n",
    "    if verbose is True:\n",
    "        print(\"Model Summary\")\n",
    "        print(\"===================\")\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_CNN(X, Y, ntrain, nval, output=\"test\", verbose=False):\n",
    "    '''Train the CNN given a dataset and output model and weights'''\n",
    "\n",
    "    # train params - hardcoded for simplicity\n",
    "    batch_size = 30\n",
    "    nb_epoch = 50\n",
    "    data_augmentation = True  # if True the data will be augmented at every iteration\n",
    "\n",
    "    ind = random.sample(range(0, ntrain+nval-1), ntrain+nval-1)\n",
    "    X_train = X[ind[0:ntrain], :, :, :]\n",
    "    X_val = X[ind[ntrain:ntrain+nval], :, :, :]\n",
    "    Y_train = Y[ind[0:ntrain]]\n",
    "    Y_val = Y[ind[ntrain:ntrain+nval]]\n",
    "\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = X_train.shape[1:3]\n",
    "    img_channels = 3\n",
    "\n",
    "    # Right shape for X\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols,\n",
    "                              img_channels)\n",
    "    X_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, img_channels)\n",
    "\n",
    "    # Avoid more iterations once convergence\n",
    "    patience_par = 10\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', patience=patience_par,\n",
    "                                  verbose=0, mode='auto' )\n",
    "    modelcheckpoint = ModelCheckpoint(output+\"_best.hd5\", monitor='val_loss',\n",
    "                                      verbose=0, save_best_only=True)\n",
    "\n",
    "    # Define CNN\n",
    "    model = CNN(img_channels, img_rows, img_cols, verbose=True)\n",
    "\n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        history = model.fit(X_train, Y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            nb_epoch=nb_epoch,\n",
    "                            validation_data=(X_val, Y_val),\n",
    "                            shuffle=True, verbose=verbose,\n",
    "                            callbacks=[earlystopping, modelcheckpoint])\n",
    "    else:\n",
    "        print('Using real-time data augmentation.')\n",
    "        # this will do preprocessing and realtime data augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,\n",
    "            samplewise_center=False,\n",
    "            featurewise_std_normalization=False,\n",
    "            samplewise_std_normalization=False,\n",
    "            zca_whitening=False,\n",
    "            rotation_range=45,\n",
    "            width_shift_range=0.05,\n",
    "            height_shift_range=0.05,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            zoom_range=[0.75, 1.3])\n",
    "\n",
    "        datagen.fit(X_train)\n",
    "\n",
    "        history = model.fit_generator(\n",
    "            datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "            samples_per_epoch=X_train.shape[0],\n",
    "            nb_epoch=nb_epoch,\n",
    "            validation_data=(X_val, Y_val),\n",
    "            callbacks=[earlystopping, modelcheckpoint])\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    # save weights\n",
    "    model.save_weights(output+\".weights\", overwrite=True)\n",
    "\n",
    "\n",
    "def apply_CNN(X, model_name):\n",
    "    '''Apply a CNN to a data set'''\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = X.shape[1:3]\n",
    "    img_channels = 3\n",
    "    X = X.reshape(X.shape[0], img_rows, img_cols, img_channels)\n",
    "\n",
    "    # load model & predict\n",
    "    print(\"Loading weights\", model_name)\n",
    "\n",
    "    model = CNN(img_channels, img_rows, img_cols)\n",
    "    model.load_weights(model_name+\".weights\")\n",
    "    Y_pred = model.predict_proba(X)\n",
    "\n",
    "    return Y_pred\n",
    "\n",
    "\n",
    "def add_titlebox(ax, text):\n",
    "    '''Add an embedded title into figure panel'''\n",
    "    ax.text(.1, .85, text,\n",
    "            horizontalalignment='left',\n",
    "            transform=ax.transAxes,\n",
    "            bbox=dict(facecolor='white', edgecolor='none', alpha=0.8))\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_CNN_performance(pred, labels):\n",
    "    '''Plot ROC curve and sample galaxies'''\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.1,\n",
    "                        left=0.1, right=0.95,\n",
    "                        bottom=0.15, top=0.9)\n",
    "\n",
    "    # define shape of figure\n",
    "    gridsize = (2, 4)\n",
    "    ax1 = plt.subplot2grid(gridsize, (0, 0), colspan=2, rowspan=2)\n",
    "    ax2 = plt.subplot2grid(gridsize, (0, 2))\n",
    "    ax3 = plt.subplot2grid(gridsize, (0, 3))\n",
    "    ax4 = plt.subplot2grid(gridsize, (1, 2))\n",
    "    ax5 = plt.subplot2grid(gridsize, (1, 3))\n",
    "\n",
    "    # plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(labels, pred)\n",
    "\n",
    "    ax1.plot(fpr, tpr, color='black')\n",
    "    ax1.set_xlabel(r'False Positive Rate')\n",
    "    ax1.set_ylabel(r'True Positive Rate')\n",
    "\n",
    "    # array of objects (good E, good S, bad E, bad S)\n",
    "    goodE = np.where((pred[:, 0] < 0.5) & (labels == 0))\n",
    "    goodS = np.where((pred[:, 0] > 0.5) & (labels == 1))\n",
    "    badE = np.where((pred[:, 0] < 0.5) & (labels == 1))\n",
    "    badS = np.where((pred[:, 0] > 0.5) & (labels == 0))\n",
    "\n",
    "    ax2.imshow(D[pred_index + goodE[0][1]])\n",
    "    add_titlebox(ax2, \"Correct E\")\n",
    "    ax2.axis('off')\n",
    "\n",
    "    ax3.imshow(D[pred_index + goodS[0][4]])\n",
    "    add_titlebox(ax3, \"Correct Spiral\")\n",
    "    ax3.axis('off')\n",
    "\n",
    "    ax4.imshow(D[pred_index + badE[0][1]])\n",
    "    add_titlebox(ax4, \"Incorrect E\")\n",
    "    ax4.axis('off')\n",
    "\n",
    "    ax5.imshow(D[pred_index + badS[0][3]])\n",
    "    add_titlebox(ax5, \"Incorrect Spiral\")\n",
    "    ax5.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "n_objects = 500\n",
    "save_files = \"./SDSS{}\".format(n_objects)\n",
    "\n",
    "# Read SDSS images and labels\n",
    "D = read_savefile(\"sdss_images_1000.npy\")[0:n_objects]\n",
    "Y = read_savefile(\"sdss_labels_1000.npy\")[0:n_objects]\n",
    "\n",
    "# Train network and output to disk (keep 10% of data for test set)\n",
    "ntrain = D.shape[0] * 8 // 10.\n",
    "nval = D.shape[0] // 10\n",
    "npred = D.shape[0] - (ntrain + nval)  # test sample size;\n",
    "pred_index = ntrain + nval            # test sample start index;\n",
    "\n",
    "# Normalize images\n",
    "mu = np.amax(D, axis=(1, 2))\n",
    "for i in range(0, mu.shape[0]):\n",
    "    D[i, :, :, 0] = D[i, :, :, 0] / mu[i, 0]\n",
    "    D[i, :, :, 1] = D[i, :, :, 1] / mu[i, 1]\n",
    "    D[i, :, :, 2] = D[i, :, :, 2] / mu[i, 2]\n",
    "\n",
    "# change order so that we do not use always the same objects to train/test\n",
    "D, Y, = shuffle(D, Y, random_state=0)\n",
    "\n",
    "my_file = Path(save_files + \".weights\")\n",
    "if my_file.is_file():\n",
    "    Y_pred = apply_CNN(D[pred_index:pred_index + npred, :, :, :], save_files)\n",
    "    Y_test=Y[pred_index:pred_index + npred]\n",
    "else:\n",
    "    print(\"Training Model\")\n",
    "    print(\"====================\")\n",
    "    model_name = train_CNN(D, Y, ntrain, nval, output=save_files)\n",
    "    Y_pred = apply_CNN(D[pred_index:pred_index + npred, :, :, :], save_files)\n",
    "    Y_test = Y[pred_index:pred_index + npred]\n",
    "\n",
    "Y_pred_class = Y_pred * 0\n",
    "Y_pred_class[Y_pred > 0.5] = 1\n",
    "print(\"Global Accuracy:\", accuracy_score(Y_test, Y_pred_class))\n",
    "\n",
    "\n",
    "plot_CNN_performance(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Autoencoders\n",
    "\n",
    "Autoencoders are neural networks that copy their input to their output, but after passing the data through a bottleneck.  For example if there are 28x28 = 784 inputs, there will also be 784 outputs, but there will be one or more (odd, but symmetric) hidden layers with fewer neurons than that.\n",
    "\n",
    "For example see \n",
    "\n",
    "![autoencoder structure](https://miro.medium.com/max/1400/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png)\n",
    "\n",
    "from\n",
    "https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "So, distill our MNIST digits down to the three most important pixels.  Or perhaps the three most important line segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "You can think of this as doing PCA with a neural network -- breaking our data down into the only the most important features that we actually *need* (finding the intrinsic dimensionality).   In fact, if the network uses only linear (or no) activation functions and $L2$ cost function, then we have exactly PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "How might this be useful?  Well, for example we can use it to reconstruct MNIST digits that have had noise added to them:\n",
    "\n",
    "![autoencoder example](https://miro.medium.com/max/1400/1*SxwRp9i23OM0Up4sEze1QQ@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "More exciting possibilities are: \n",
    "\n",
    "- Using autoencoders for \"**unsupervised pretraining**\". For example you have data that is only partially labeled (at least not enough to do traditional supervised classification).  We can train an autoencoder on the full data set, then used the encoder part as the base of a regular neural network that is trained on the labeled data that we do have. This is found to be a much more efficient way of initializing weights and biases than starting from random, because you are \"cheating\" a bit by already figuring out some of the data structure using the autoencoder. See Geron Figure 17.6.\n",
    "- **[Anomaly detection](https://scikit-learn.org/stable/modules/outlier_detection.html)**, e.g. https://towardsdatascience.com/anomaly-detection-with-autoencoder-b4cdce4866a6 and https://towardsdatascience.com/anomaly-detection-using-autoencoders-5b032178a1ea. See also\n",
    "https://www.pyimagesearch.com/2020/03/02/anomaly-detection-with-keras-tensorflow-and-deep-learning/ and \n",
    "https://towardsdatascience.com/autoencoder-neural-network-for-anomaly-detection-with-unlabeled-dataset-af9051a048."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "I'm not saying that you are ready for a data science job after these two lectures, but you now know more than the majority of astrophysicists that talk about neural networks, and you have used the two big codes. There are tons of useful videos, tutorials, and online courses that can take you further if you are interested in deep learning.\n",
    "\n",
    "Another tool you may be interested in: https://astronn.readthedocs.io/en/latest/index.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "nbpresent": {
   "slides": {
    "a6340146-092b-47d0-8584-e84bb64c0952": {
     "id": "a6340146-092b-47d0-8584-e84bb64c0952",
     "prev": null,
     "regions": {
      "9f3c3dc8-1276-4b96-9b97-e4b352cc7fb5": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0.008349570712902259,
        "y": -0.008482103581361025
       },
       "id": "9f3c3dc8-1276-4b96-9b97-e4b352cc7fb5"
      }
     }
    }
   },
   "themes": {}
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
